{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=r'''C:\\Users\\Nandit\\Desktop\\BDRW_train\\BDRW_train_1'''\n",
    "name=r'''/digit_0.jpg'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input=train_dir+name\n",
    "img=Image.open(Input)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>digit_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digit_1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>digit_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>digit_3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>digit_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1\n",
       "0  digit_0  1\n",
       "1  digit_1  4\n",
       "2  digit_2  2\n",
       "3  digit_3  3\n",
       "4  digit_4  1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=r'''C:\\Users\\Nandit\\Desktop\\BDRW_train\\BDRW_train_1\\labels.xls'''\n",
    "data=pd.read_excel(path, header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={ 0:'filename', 1:'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>digit_0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digit_1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>digit_2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>digit_3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>digit_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  class\n",
       "0  digit_0      1\n",
       "1  digit_1      4\n",
       "2  digit_2      2\n",
       "3  digit_3      3\n",
       "4  digit_4      1"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: digit_0\n"
     ]
    }
   ],
   "source": [
    "print('File name:', data.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1\n"
     ]
    }
   ],
   "source": [
    "print('Class:', data.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows:  1393\n"
     ]
    }
   ],
   "source": [
    "print('The number of rows: ', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms \n",
    "#For installing torchvision :-pip install https://files.pythonhosted.org/packages/8c/52/33d739bcc547f22c522def535a8da7e6e5a0f6b98594717f519b5cb1a4e1/torchvision-0.1.8-py2.py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, excel_file, data_dir, transform=None):\n",
    "        \n",
    "        # Image directory\n",
    "        self.data_dir=data_dir\n",
    "        \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the CSV file contians image info\n",
    "        self.data_name= data\n",
    "        \n",
    "        # Number of images in dataset\n",
    "        self.len=self.data_name.shape[0] \n",
    "    \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Image file path\n",
    "        img_name=self.data_dir + self.data_name.iloc[idx, 0]+\".jpg\"\n",
    "        \n",
    "        # Open image file\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        # The class label for the image\n",
    "        y = self.data_name.iloc[idx, 1]\n",
    "        \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(excel_file=data\n",
    "                        , data_dir=r'''C:\\Users\\Nandit\\Desktop\\BDRW_train\\BDRW_train_1/''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, y = train_dataset.__getitem__(1)\n",
    "plt.imshow(image)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'RandomGrayscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-3e52f7291057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.485\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.456\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.406\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.229\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.225\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m composed = transforms.Compose([transforms.RandomGrayscale(p=0.1), transforms.Scale((224, 224)),\n\u001b[0m\u001b[0;32m      6\u001b[0m                                transforms.ToTensor(), transforms.Normalize(mean, std)]) #transforms.Resize not working\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'RandomGrayscale'"
     ]
    }
   ],
   "source": [
    "# Create the transform compose\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "composed = transforms.Compose([transforms.RandomGrayscale(p=0.1), transforms.Scale((224, 224)),\n",
    "                               transforms.ToTensor(), transforms.Normalize(mean, std)]) #transforms.Resize not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(excel_file=data\n",
    "                        , data_dir=r'''C:\\Users\\Nandit\\Desktop\\BDRW_train\\BDRW_train_1/'''\n",
    "                        , transform = composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_dir=r'''C:\\Users\\Nandit\\Desktop\\BDRW_train\\BDRW_val/'''\n",
    "validation_dataset = Dataset(transform=composed\n",
    "                          ,excel_file=data\n",
    "                          ,data_dir=validation_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x225212a5dd0>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch modules\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "#non-pytorch modules\n",
    "import time\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import random\n",
    "from matplotlib.pyplot import imshow\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\Nandit/.cache\\torch\\checkpoints\\resnet18-5c106cde.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "#step 1:Load pretrained model resnet18\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 3, 5, 6, 7, 8, 9, 0], dtype=int64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Re-defined the last layer\n",
    "model.fc=nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the data loader\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=15)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the pre-defined optimizer Adam with learning rate 0.003\n",
    "optimizer=torch.optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad],lr=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-b052f4c5903e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mloss_sublist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-179-1be881883bcf>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# If there is any transform method, apply it onto the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the model\n",
    "\n",
    "N_EPOCHS = 20\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "correct = 0\n",
    "n_test = len(validation_dataset)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss_sublist=[]\n",
    "    for x,y in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z=model(x)\n",
    "        loss=criterion(z,y)\n",
    "        loss_sublist.append(loss.data.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(np.mean(loss_sublist))\n",
    "        \n",
    "        correct=0\n",
    "        for x_test,y_test in validation_loader:\n",
    "            model.eval()\n",
    "            z=model(x_test)\n",
    "            _,yhat=torch.max(z.data,1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "            accuracy = correct/n_test\n",
    "            accuracy_list.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-9c5fa846d41d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision' has no attribute '__version__'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
